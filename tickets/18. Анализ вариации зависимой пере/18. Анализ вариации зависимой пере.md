# 18. Анализ вариации зависимой переменной в модели парной линейной регрессии.. Проверка качества уравнения регрессии в целом.

В модели парной линейной регрессии $\hat{y} = b_0 + b_1 x$ важным этапом является оценка того, насколько хорошо
полученное уравнение описывает реальные данные.

## 1. Анализ вариации зависимой переменной (Разложение суммы квадратов)

Общая вариация зависимой переменной $y$ складывается из отклонений её фактических значений $y_i$ от среднего
значения $\bar{y}$. Суммарная мера этого разброса называется общей суммой квадратов отклонений (**TSS** — Total Sum of
Squares).

В соответствии с принципами регрессионного анализа, общая сумма квадратов отклонений разбивается на две части:

1. **ESS** (Explained Sum of Squares) — сумма квадратов отклонений, объясненная регрессией (влиянием фактора $x$).
2. **RSS** (Residual Sum of Squares) — остаточная сумма квадратов отклонений, обусловленная влиянием прочих случайных
   факторов.

### Основное тождество:

$$TSS = ESS + RSS$$
$$\sum (y_i - \bar{y})^2 = \sum (\hat{y}_i - \bar{y})^2 + \sum (y_i - \hat{y}_i)^2$$

**TSS (Total Sum of Squares)** - Вариация (дисперсия). Сумма квадратов отклонений фактических значений от
среднего ($\sum (y_i - \bar{y})^2$) — это и
есть **TSS**.

**RSS (Residual Sum of Squares)** — $S_e^2 = \frac{\sum e^2}{n-2}$. Величина $\sum e^2$ (сумма квадратов остатков
регрессии) — это и есть **RSS**.

**ESS (Explained Sum of Squares)** — это разность между общей и остаточной суммами квадратов. Она напрямую используется
в
ваших файлах для расчета коэффициента детерминации ($R^2$) и F-критерия Фишера.

### Вывод формулы (дополнено на основе общих принципов регрессии, упомянутых в файлах):

Любое фактическое значение можно представить как $y_i = \hat{y}_i + e_i$. Тогда отклонение от среднего:
$$(y_i - \bar{y}) = (\hat{y}_i - \bar{y}) + (y_i - \hat{y}_i)$$
При возведении в квадрат и суммировании по всем наблюдениям:
$$\sum (y_i - \bar{y})^2 = \sum (\hat{y}_i - \bar{y})^2 + \sum (y_i - \hat{y}_i)^2 + 2\sum (\hat{y}_i - \bar{y})(y_i - \hat{y}_i)$$
В методе наименьших квадратов (МНК) сумма произведений отклонений и остатков $2\sum (\hat{y}_i - \bar{y})e_i$ равна
нулю, что и дает итоговое разложение.

---

## 2. Коэффициент детерминации ($R^2$)

**Коэффициент детерминации** — это основная мера качества уравнения регрессии, показывающая долю вариации зависимой
переменной, которая объясняется влиянием независимого фактора $x$.

Формула расчета:
$$R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$$

* В случае парной линейной регрессии коэффициент детерминации равен квадрату линейного коэффициента корреляции
  Пирсона: $R^2 = r_{xy}^2$.
* Значение $R^2$ лежит в пределах от 0 до 1. Чем ближе оно к 1, тем выше качество модели.

---

## 3. Проверка качества уравнения в целом (F-критерий Фишера)

Для проверки статистической значимости всего уравнения регрессии используется **дисперсионное отношение Фишера** ($F$
-критерий). Это позволяет проверить, является ли найденная связь между переменными случайной или она статистически
значима.

### Статистика критерия:

Согласно документу «Прогнозирование в регрессионных моделях», наблюдаемое значение $F$-критерия вычисляется через
коэффициент детерминации:
$$F = \frac{R^2}{1 - R^2} (n - 2)$$
где:

* $R^2$ — коэффициент детерминации;
* $n$ — объем выборки;
* $(n - 2)$ — число степеней свободы для остаточной суммы квадратов в парной регрессии.

### Алгоритм проверки:

1. **Выдвижение гипотез:**
    * $H_0: b_1 = 0$ (уравнение незначимо, влияние фактора $x$ отсутствует).
    * $H_1: b_1 \neq 0$ (уравнение значимо).
2. **Определение критического значения:** По таблицам распределения Фишера находится $F_{крит}$ для заданного уровня
   значимости $\alpha$ и степеней свободы $k_1 = 1$ (для одного фактора) и $k_2 = n - 2$.
3. **Принятие решения:**
    * Если $F_{набл} > F_{крит}$, то гипотеза $H_0$ отклоняется, и уравнение регрессии признается **статистически
      значимым**.
    * Если $F_{набл} < F_{крит}$, то качество модели признается неудовлетворительным, а связь — незначимой.

---

## 4. Дополнительные критерии качества

### Средняя ошибка аппроксимации (MAPE)

Используется для оценки относительной точности модели. Вычисляется как среднее относительное отклонение расчетных
значений от фактических:
$$MAPE = \frac{1}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right| \cdot 100\%$$

* **Менее 10%** — высокая точность.
* **10–20%** — хорошая точность.
* **20–50%** — удовлетворительная точность.

### Остаточная дисперсия

Оценка вариации случайной ошибки (остатков) вычисляется по формуле:
$$S_e^2 = \frac{\sum e^2}{n - 2}$$
где $\sum e^2$ — сумма квадратов остатков ($RSS$).