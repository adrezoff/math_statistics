# 28. Метод главных компонент. Модель, математическое обоснование и вычисление главных компонент.

**Сущность метода главных компонент**

В исследовательской работе приходится сталкиваться с ситуациями, когда общее число р признаков $X_{1},X_{2},...,X_{p}$ регистрируемых на множестве объектов очень велико - 100 и более. Но, тем не менее, имеющиеся многомерные наблюдения надо подвергнуть статистической обработке, осмыслить, ввести в базу данных для того, чтобы иметь возможность использовать эти данные в нужный момент.

Таким образом, возникает задача «сжатия» информации: представление каждого наблюдения не р признаками, а меньшим числом показателей $F=(f_{1},f_{2},...,f_{p^{\prime}})$, где $p^{\prime}<p$. Для этого есть несколько причин:

* Необходимость наглядного представления (визуализации) исходных данных - проекция $X_{(p)}$ на трехмерное $p^{\prime}=3$ или двухмерное $p^{\prime}=2$ пространство;
* Стремление к лаконизму моделей, необходимому для упрощения расчетов и интерпретации полученных результатов;
* Необходимость сжатия объема хранимой информации без ощутимой потери ее информативности.

В зависимости от требований к новой системе признаков $f_{i}$ приходят к тому или иному алгоритму снижения размерности. Имеется три типа принципиальных предпосылок, которые обуславливают возможность перехода от большого числа р исходных показателей к существенно меньшему числу $p^{\prime}$ наиболее информативных переменных:

* дублирование информации, доставляемой сильно взаимосвязанными признаками;
* неинформативность признаков, мало меняющихся при переходе от одного объекта к другому (малая «вариабельность» признаков);
* возможность агрегирования, т.е. простого или «взвешенного» суммирования, по некоторым признакам

Формально задача перехода (с наименьшими потерями в информативности) к новому набору признаков $F=(f_{1},f_{2},...,f_{p^{\prime}})$ может быть описана следующим образом:

Пусть $F=F(X)=(f_{1},f_{2},...,f_{p^{\prime\prime}})-$ некоторая $p^{\prime}$-мерная вектор-функция от исходных переменных $X=(X_{1},X_{2},...,X_{p})$ $(p<p^{\prime})$ и пусть $I_{p^{\prime}}[F(X)]-$ определенным образом заданная мера информативности $\overline{p^{\prime}}$-мерной системы признаков $F(X)$. Конкретный выбор функционала $I_{p^{\prime}},[F]$ зависит от поставленной задачи. Например, максимальное сохранение информации, содержащейся в исходной системе показателей X.

Математическая модель, лежащая в основе любого метода снижения размерности включает три компоненты:
1. Форма задания исходной информации.
2. Класс $G(X)$ допустимых преобразований исходных признаков. От выбора $G(X)$ существенно будет зависеть $F(X)$. Большинство используемых на практике преобразований линейные модели, подходящим образом нормированные.
3. Тип опримизируемого критерия $I_{p^{\prime}}[F(X)]$. Существуют критерии автоинформативности, которые позволяют перейти к $F(X_{p^{\prime}})$ и максимально точно воспроизводить информацию, содержащуюся в массиве исходных данных (восстановление $[n\times p]$ значений исходных признаков Х по $[n\times p^{\prime}]$ значениям $F(X_{p^{\prime}}))$.

Рассмотрим по порядку все эти компоненты для метода главных компонент.

1. Будем считать, что исходные данные представляют собой матрицы типа «объект-свойство» или «наблюдение-свойство»

$$X=\begin{pmatrix}x_{11}&x_{12}&...&x_{1p};x_{21}&x_{22}&....&x_{2p};.........................;x_{n1}&x_{n2}&...&x_{np}\end{pmatrix}, \quad$$

где $x_{ij}-$ значение ј-го анализируемого признака, характеризующего состояние і-го объекта, $j=\overline{1,p},$ i = 1,п, $p-$ число признаков, $n-$ число исследуемых объектов. Столбцы этой матрицы представляют признаки $X_{j}$ зафиксированные на п объектах, а строки характеристики конкретного объекта по всем р показателям.

Наилучший результат метода главных компонент достигается, когда все компоненты Х измерены в одних и тех же единицах. Если Х измерены в разных единицах, то результаты будут зависеть от выбора масштаба и природы единиц измерения. Поэтому лучше перейти к безразмерным переменным, подходящим образом нормируя исходные признаки. На практике обычно исходные данные стандартизуют:

$$z_{ij}=\frac{x_{ij}-\overline{X}_{j}}{\sqrt{\sigma_{jj}}}, \quad$$

где $\overline{X_{j}}$ - среднее значение признака $X_{j}$, $\sigma_{jj}$ - дисперсия признака $X_{j}$.

При таком преобразовании математическое ожидание и дисперсия преобразованных данных $M(Z_{j})=0$ и $D(Z_{j})=1$, а ковариационная матрица $S(Z)$ является матрицей корреляций $R(X)$ исходных данных Х:

$$cov(Z_{k},Z_{j})=M\left[\left(\frac{X_{k}-M(X_{k})}{\sqrt{\sigma_{kk}}}-M(Z_{k})\right)\left(\frac{X_{j}-M(X_{j})}{\sqrt{\sigma_{jj}}}-M(Z_{j})\right)\right]=$$$$=\frac{1}{\sqrt{\sigma_{kk}\sigma_{jj}}}M[(X_{k}-M(X_{k}))(X_{j}-M(X_{j})]=\frac{\sigma_{kj}}{\sqrt{\sigma_{kk}\sigma_{jj}}}=r_{kj}.$$

При этом ковариационную матрицу стандартизованных данных можно представить в виде:

$$S(Z)=S_{z}=\frac{1}{n}Z^{T}\cdot Z=R_{x} \quad$$

2. Определим $G(X)$ как всевозможные линейные ортогональные нормированные комбинации исходных переменных Х, или, если используем стандартизованные данные, то переменных Z:

$$G=\{F:f_{j}=\sum_{i=1}^{p}u_{ij}Z_{j},j=\overline{1,p}\}$$

С дополнительными требованиями - нормировки и ортогональности для коэффициентов U:

$$\sum_{i=1}^{p}u_{ij}^{2}=1,j=\overline{1,p} \quad \text{нормировка}$$
$$\sum_{i=1}^{\overline{p}}u_{ij}u_{ik}=0, k\ne j, i,j=\overline{1,p} \quad \text{ортогональность} \quad$$

3. В качестве критерия (меры) информативности $p^{\prime}$-мерной системы показателей $F(Z)=(f_{1},f_{2},...,f_{p^{\prime}})$ возьмем функционал:

$$I_{p^{\prime}}[F(Z)]=\frac{D(F_{1})+D(F_{2})+...+D(F_{p^{\prime}})}{D(Z_{1})+D(Z_{2})+...+D(Z_{p})}\longrightarrow max \quad$$

Этот функционал требует обеспечения максимальной величины дисперсии (вариабельности) новых переменных $F_{j}$ по отношению к исходным переменным $Z_{j}$.Таким образом, получаем задачу. Найти новые показатели F, представляющие из себя линейную комбинацию исходных переменных, т.е. определить коэффициенты этой линейной комбинации U:

$$F=ZU$$$$U=\begin{pmatrix}u_{11}&u_{12}&...&u_{1p} ; u_{21}&u_{22}&...&u_{2p} ; .................... ; u_{p1}&x_{p2}&...&u_{pp}\end{pmatrix} \quad$$

причем, векторы-столбцы матрицы обладают свойством ортогональности, и длина каждого равна единице.

Найденные таким образом новые векторы (после нормирования) $F=F(Z)=(f_{1},f_{2},...,f_{p^{\prime}})$ называют главными компонентами.

**Определение:** Первой главной компонентой $f_{1}(Z)$ исследуемой системы показателей $Z=(Z_{1},Z_{2},...,Z_{p})$ называется такая нормированно-центрированная линейная комбинация этих показателей, которая обладает наибольшей дисперсией.

**Определение:** k-й главной компонентой $f_{k}(Z)$, $k=2,3,...,p$ исследуемой системы показателей $Z=(Z_{1},Z_{2},...,Z_{p})$ называется такая линейная комбинация этих показателей, котораяне коррелирует с $(k-1)$ предыдущими главными компонентами; обладает наибольшей дисперсией среди $(p-k+1)$ других главных компонент. 

Соответственно, второй главной компонентой $f_{2}$ будем называть линейную комбинацию исходных показателей, имеющую вторую по величине дисперсию, плюс к этому, вторая главная компонента не должна коррелировать с первой и т.д. к-я главная компонента имеет к-ю по величине дисперсию и не коррелирует с предыдущими вычисленными главными компонентами $f_{1},f_{2},...,f_{k-1}.$ Максимальное количество полученных главных компонент равно числу исходных показателей р.

**Вычисление главных компонент**

Из определения главных компонент следует, что для вычисления первой главной компоненты необходимо решить задачу оптимизации:

$$\begin{cases}D(Z\cdot U_{1})\longrightarrow max,
U_{1}^{T}U_{1}=1,\end{cases} \quad$$

здесь $U_{1}$ - первый столбец матрицы U, второе выражение отражает условие нормировки (длина вектора $U_{1}$ равна 1).

Вычислим дисперсию $D(Z\cdot U_{1})$:

$$D(ZU_{1})=M(ZU_{1})^{2}=M[(ZU_{1})^{T}(ZU_{1})]=M[U_{1}^{T}Z^{T}ZU_{1}]=U_{1}^{T}M(Z^{T}Z)U_{1}=U_{1}^{T}S_{z}U_{1}=U_{1}^{T}R_{x}U_{1}.$$

Тогда задача примет вид:

$$\begin{cases}U_{1}^{T}R_{x}U_{1}\longrightarrow max,  
U_{1}^{T}U_{1}=1,\end{cases} \quad (9)$$

Необходимо решить задачу на условный экстремум. Составим функцию Лагранжа:

$$\varphi(U_{1},\lambda)=U_{1}^{T}R_{x}U_{1}-\lambda(U_{1}^{T}U_{1}-1).$$

Продифференцируем $\varphi(U_{1},\lambda)$ по вектору столбцу $U_{1}$, и для выполнения необходимого условия экстремума приравняем эту производную к нулю:

$$\frac{\partial\varphi}{\partial U_{1}}=2R_{x}U_{1}-2\lambda U_{1}=0.$$

Получили в результате систему линейных однородных уравнений

$$R_{x}U_{1}-\lambda U_{1}=0, \text{ или } (R_{x}-\lambda I)U_{1}=0, \quad$$

здесь I - единичная матрица размера р, а 0 нулевой вектор размерности $[p\times1]$.

Для того, чтобы эта система однородных линейных уравнений имела ненулевое решение, ее матрица должна быть вырожденной, т.е. ее определитель должен быть равен нулю.

$$|R_{x}-\lambda I|=0.$$

Получили задачу на определение собственных значений матрицы $R_{x}$. Вычислив определитель, получим уравнение степени р относительно $\lambda$, т.е. характеристическое уравнение для матрицы $R_{x}$. Так как матрица $R_{x}$ симметричная и неотрицательно определенная, то характеристическое уравнение будет иметь р вещественных неотрицательных корней $\lambda_{1}\ge \lambda_{2}\ge...\lambda_{p}\ge0$.

Дисперсия первой главной компоненты, определенная нами ранее равна $D(f_{1})=D(ZU_{1})=U_{1}^{T}R_{x}U_{1}$.

С другой стороны из уравнения следует, что:

$$R_{x}U_{1}-\lambda U_{1}=0,$$

$$R_{x}U_{1}=\lambda U_{1}, \text{ домножим слева на } U_{1}^{T},$$

$$U_{1}^{T}R_{x}U_{1}=U_{1}^{T}\lambda U_{1}$$

$$U_{1}^{T}R_{x}U_{1}=\lambda U_{1}^{T}U_{1}, \quad (U_{1}^{T}U_{1}=1).$$

$$U_{1}^{T}R_{x}U_{1}=\lambda$$

Получили, что $D(f_{1})=\lambda$. Поэтому для обеспечения максимальной величины дисперсии первой главной компоненты $f_{1}$ надо выбрать из р собственных значений матрицы $R_{x}$ наибольшее, то есть $\lambda_{1}$. Таким образом, $D(f_{1})=\lambda_{1}$.

Подставив $\lambda_{1}$ в систему уравнений, решив ее, определим компоненты вектора $U_{1}$ - собственного вектора матрицы $R_{x}$.

Вывод: Первая главная компонента получается как линейная комбинация $f_{1}=ZU_{1}$ где $U_{1}$ собственный вектор матрицы $R_{x}$, соответствующего первому (наибольшему) собственному числу $\lambda_{1}$.

Далее, аналогично можно показать, что любая к-я главная компонента $f_{k}=ZU_{k}$ где $U_{k}$ собственный вектор матрицы $R_{x}$ соответствующего собственному числу $\lambda_{k}$. Причем дисперсия этой компоненты $D(f_{k})=\lambda_{k}$.

Другими словами, надо решить р систем однородных уравнений для каждого значения $\lambda_{j}$, $j=\overline{1,p}.$ Таким образом определим р собственных векторов:

$$(R_{x}-\lambda_{j}I)V_{j}=0.$$

В развернутом виде эта система имеет вид:



Число уравнений системы равно числу неизвестных. Поэтому она имеет бесконечно много решений. Конкретные значения собственных векторов при этом можно найти, задавая произвольно по крайней мере величину одной компоненты каждого вектора. Обычно, чтобы не усложнять расчеты, ее приравнивают единице. Заметим, что при этом ничто не гарантирует нам, что собственный вектор будет нормированым (т.е. его длина не равна единице). Поэтому после решения систем уравнений (10) необходимо нормировать полученные собственные векторы:

$$U_{j}=\frac{V_{j}}{|V_{j}|}$$

Таким образом, мы получим матрицу U, столбцами которой являются собственные векторы матрицы корреляций. Причем, эти векторы ортогональны $(U_{k}^{T}\cdot U_{j}=0 \text{ при } k\ne j)$ и нормированы $(U_{j}^{T}\cdot U_{j}=1 \text{ - длина вектора})$.

Теперь для каждого объекта можем вычислить его новые координаты, используя выражение

$$F=ZU,$$

где F - матрица размерности $(n\times p)$ элементы которой $f_{ij}$ представляют собой значения ј-й главной компоненты для і-го объекта (наблюдения).
