# 33. Метод К – средних в кластерном анализе. Положительные и отрицательные стороны этого метода.

### 1. Понятие метода К-средних
**Метод К-средних** является наиболее популярным методом **неиерархической** кластеризации. Он относится к классу итерационных методов разбиения, при которых объекты перемещаются из одного кластера в другой до тех пор, пока не будет достигнуто наиболее «удачное» разбиение. 

Целью алгоритма является минимизация **суммарного квадратичного отклонения** точек кластеров от центров этих кластеров. В результате работы метода $n$ объектов распределяются по $k$ заранее заданным кластерам таким образом, чтобы каждый объект относился к тому кластеру, к центру (центроиду) которого он находится ближе всего.

**Математическое обоснование:** Алгоритм стремится минимизировать целевую функцию — суммарное квадратичное отклонение точек от центроидов соответствующих кластеров (SSE — Sum of Squared Errors):
$$J = \sum_{i=1}^{k} \sum_{x \in S_i} ||x - \mu_i||^2$$
Где:
* $k$ — заданное количество кластеров;
* $S_i$ — сформированные кластеры;
* $\mu_i$ — центроиды (векторы средних значений) кластеров.

Геометрически метод разбивает пространство признаков на полигоны (ячейки) Вороного: каждая точка внутри такого полигона находится ближе к его центроиду, чем к любому другому.

### 2. Алгоритм метода
Процедура метода К-средних состоит из следующих основных этапов:

1.  **Выбор количества кластеров ($k$):** Исследователь заранее определяет, на сколько групп необходимо разбить совокупность данных.
2.  **Выбор начальных центров:** В качестве начальных центров кластеров выбираются $k$ точек в многомерном пространстве признаков. Это могут быть либо первые $k$ наблюдений, либо объекты, выбранные случайным образом, либо точки, максимально удаленные друг от друга.
* Классический подход: случайный выбор или первые $k$ объектов.
* Продвинутый подход (K-means++): первый центр выбирается случайно, а каждый следующий — с вероятностью, пропорциональной квадрату расстояния до уже выбранных. Это ускоряет сходимость и снижает риск попадания в плохой локальный минимум.
3.  **Распределение объектов по кластерам:** Для каждого объекта выборки рассчитывается расстояние (обычно Евклидово) до всех начальных центров. Объект приписывается к тому кластеру, расстояние до центра которого минимально.
4.  **Пересчет центров:** После того как все объекты распределены, вычисляются новые координаты центров кластеров как **средние арифметические** координат всех объектов, вошедших в данный кластер.
5.  **Итерационный процесс:** Шаги 3 и 4 повторяются. На каждой итерации объекты могут менять свою принадлежность к кластерам, а центры кластеров — перемещаться. Алгоритм завершается, когда объекты перестают переходить из кластера в кластер или когда изменения координат центров становятся незначительными.

### 2.5. Определение оптимального числа кластеров ($k$)
Поскольку $k$ нужно знать заранее, на практике используют:
* **Метод «локтя» (Elbow Method):** Анализ графика зависимости SSE от числа кластеров. Точка резкого замедления падения ошибки («изгиб локтя») указывает на оптимальное $k$.
* **Метод силуэтов (Silhouette Method):** Оценка того, насколько объект похож на свой кластер по сравнению с соседними. Значение близкое к 1 означает качественную классификацию.

### 3. Положительные стороны метода
*   **Высокая эффективность и скорость:** Метод работает значительно быстрее иерархических процедур и позволяет обрабатывать очень большие массивы данных.
*   **Простота реализации:** Алгоритм легко программно реализуется и понятен для интерпретации результатов.
*   **Гарантированная сходимость:** Процедура всегда сходится к некоторому локальному минимуму суммы квадратов отклонений.
*   **Доступность в ПО:** Метод реализован во всех ведущих статистических пакетах, включая Statistica.

### 4. Отрицательные стороны метода
*   **Необходимость задания $k$:** Исследователь должен знать или предполагать число кластеров до начала анализа, что не всегда возможно.
*   **Зависимость от начальных центров:** Результат кластеризации сильно зависит от того, какие точки были выбраны в качестве «стартовых» центров. При разных начальных условиях можно получить разные итоговые разбиения.
*   **Чувствительность к выбросам:** Наличие аномальных значений (шума) в данных существенно искажает расчет средних координат (центроидов).
*   **Ограничение по форме кластеров:** Метод эффективно находит только кластеры выпуклой, близкой к сферической форме. Если кластеры имеют сложную, вытянутую или «вложенную» структуру, метод К-средних может не справиться с их выделением.
* **Локальные минимумы:** Алгоритм не гарантирует нахождение самого лучшего (глобально оптимального) разбиения, он находит лишь «достаточно хорошее» решение.

4. Положительные и отрицательные стороныПоложительные стороны (Плюсы)Отрицательные стороны (Минусы)Скорость: Эффективен на больших выборках ($n > 10 000$), где иерархические методы бессильны.Априорное $k$: Необходимо заранее знать количество групп.Простота: Легко интерпретируется как поиск «типичных представителей» (средних).Чувствительность к инициализации: Случайный старт может привести к разным результатам.Динамичность: В отличие от иерархических методов, объекты могут менять свои группы в процессе пересчета.Чувствительность к выбросам: Аномалии сильно смещают среднее значение (центроид).Сходимость: Алгоритм всегда находит решение (хотя и локально оптимальное).Форма кластеров: Находит только выпуклые, сферические структуры; бессилен перед сложными формами.

---

**Практические рекомендации:** Для повышения качества классификации рекомендуется:
* Предварительно стандартизировать данные (привести их к единому масштабу), так как метод чувствителен к единицам измерения.
* Проводить расчет несколько раз с разными начальными центрами, чтобы убедиться в устойчивости полученных групп.
* Использовать дополнительные критерии (например, «метод локтя») для обоснования выбора числа $k$.