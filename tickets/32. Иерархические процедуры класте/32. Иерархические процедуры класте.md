# 32. Иерархические процедуры кластерного анализа. Меры близости между кластерами.

### ОПР (Кластерного анализа)

**Кластерный анализ** - вид многомерной классификации априорной информации о числе и типе классов, на которые разбивается совокупность объектов. Полученные в результате разбиения классы называют кластерами

Кластерный анализ применяют:
* для классификации слабоизученных явлений
* в качестве предварительного этапа обработки данных
* при проверке предположения о наличии некоторой структуры в изучаемой совокупности объектов

Методы кластерного анализа подразделяют на:
* Итеративные (k-means)
* Иерархические (метод ближней связи, метод дальней связи, метод уорда)

### Про иерархические процедуры кластерного анализа

иерархические процедуры разделяют на:
* **алгомеративные** - последовательно объединяющие сначала близкие, а затем более отдаленные друг от друга объекты в группы(кластеры)
* **дивизимные** - разделяющие сначала самые далекие группы, а затем всё более близкие друг к другу на группы на отдельные объекты

* с помощью иерархических процедур можно построить график **дендраграмму**:
* ![alt text](image.png)
  * дендрограмма для англомеративного метода кластеризации
* Требуют много вычислений, так что при большом объеме данных можно о них забыть

### Про меры близости

Данные представляются в виде матрицы
![alt text](image-1.png)

где:
* $x_{i,j}$ - значение j-ого ($j \in \{1,2,..., p\}$) анализируемого признака, характеризующего состояние i-ого ($i \in \{1,2,...,n\}$) объекта
* p - число признаков
* n - число наблюдений

Для классификации вводят понятие сходства объектов по их признакам, которое задаётся введением метрики для расчёта расстояний между объектами

**Выбор метрики это важно**

## Вектор это строка

### Обычно в качестве метрик используют:
**Обобщенное расстояние Махаланобиса**(имя у чувака конечно хайповое)
$$d_{i,k}=\sqrt{(x_i - x_k)^T \Lambda ^ T \Sigma^{-1}\Lambda(x_i-x_k)}$$
где
* $\Lambda$ - диагональная матрица весовых коэфициентов исходных показателей
* $\Sigma^{-1}$ - матрица обратная ковариационной матрице $\Sigma$
* x_i - вектор $(x_{i,1},x_{i,2}, ... , x_{i,p})^T$ столбик
  * x_k - вектор $(x_{k,1},x_{k,2}, ... , x_{k,p})^T$ cтолбик

Используют при различной значимости компонент вектора наблюдений X в решении вопроса об отнесении объекта к тому или иному классу.

**Евклидово расстояние** базированное

$$d_{i,k} = \sqrt{\sum_{j=1}^{p} (x_{i,j} - x_{k,j})^2}$$

используют если:
* признаковое пространство совпадает с геометрическим пространством
  *  значения компонент вектора имеют одинаковые порядки значений, если это не так, то кластеризация будет проходить по признаку с большими значениями, т.к они вносят больший вклад в расстояние
    * Чтобы бороться с этим используй процедуру стандартизации $z_{i,j} = \frac{x_{i,j} - \bar{x_{.,j}}}{\sigma_{x_{.,j}}}$
  * Значения компонент вектора должны быть независимыми(это прям идеально), либо слабо коррелированными, т.к в случае сильной корреляции мы будем учитывать один и тот же признак несколько раз.
    * Если это так то лучше использовать метрику Махаланобиса, т.к она учитывает коррелированность компонент
* компоненты вектора x - однородны по своему физическому смыслу и одинаково важны для классификации
  * значения компонет вектора должны иметь одинаковый смысловой вклад, иначе можно сделать не тот вывод
    * Используй взвешенную евклидову метрику, если какие-то параметры по важности преобладают над другими
  * Нужно четко понимать какой смысл несет расстояние между объектами, как можно интерпретировать более дальние или близкие объекты
* наблюдения берутся из многомерной генеральной совокупности, имеющей многомерное нормальное распределение, т.е компоненты вектора взаимно независимы и имеют одну и ту же дисперсию
  * Тут имеется в виду, что кластеры имеют форму гиперсфер, а распределение изотропно, вероятностные свойства не зависят от направления или ориентации в пространстве, т.е нет областей-сгустков и областей-пустот, симметричные формы кластеров

**Взвешенное евклидовое расстояние**

$$d_{i,k} = \sqrt{\sum_{j=1}^{p}\omega_j(x_{i,j} - x_{k,j})^2}$$

где:
* $\omega_j$ - вес j-ого показателя

Взвешенное расстояние используют, когда хотят придать какой-то компоненте вектора особую важность

**Хеммингово расстояние**

$$d_{i,k} = \sum_{j=1}^{p}|x_{i,j} - x_{k,j}|$$

Используют в основном для объектов, которые описаны бинарными признаками

**Расстояние Минковского**

$$d_{i,k} = \left(\sum_{j=1}^{p}|x_{i,j} - x_{k,j}|^p \right)^{\frac{1}{p}}$$


Как мы видим, в большинстве метрики сходство зависит от абсолютного значения признака, а также от степени его вариации в совокупности. Чтобы устранить влияние масштаба нужно провести нормировку одним из способов:
* $x_{i,j}* = \frac{x_{i,j} - \bar{x_j}}{\sigma_j}$ 
  * Z - score нормализация.
  * После преобразования все переменные имеют  $\bar{x_{i,j}*} = 0$ и $D({x_{i,j}*)} = 1$
  * исходное распределение остаётся прежним, меняется только масштаб
  * устойчивость к выбросам
* $x_{i,j}* = \frac{x_{i,j}}{\bar{x_j}}$
  * Легкая интерпретация
  * Сохранение 0
  * Чувствительность к выбросам
  * Если среднее $\approx 0$, то получаем гигантские значения
* $x_{i,j}* = \frac{x_{i,j}}{x_{max \ j}}$ 
  * данные лежат от 0 до 1
  * оставляют 0
  * чувствительно к выбросам
* $x_{i,j}* = \frac{x_{i,j}}{x_{min \ j}}$
  * сохраняет 0
  * чувствительно к выбросам
  * очень большие значения, если ${x_{min \ j}} \approx 0$, а если будет 0 то вообще None


## Про иерархические процедуры кластерного анализа

### Англомеративные методы

Суть в том, что на 1-ом шаге каждый объект выборки является кластером. Формирование кластеров происходит по этапно с помощью матрицы расстояний либо сходства объектов. На каждом шаге **Самые близкие кластеры объединяются в один**

Если в выборке n объектов, то они объединятся за n-1 шаг. Процесс объединения можно показать с помощью дендрограммы

Пусть:
* $S_m$ - m-ая группа объектов
* $n_m$ - число объектов, образующих группу S_m
* вектор $\bar{x}_m$ - среднее арифметическое векторных наблюдений, входящих в $S_m$. Т.е $\bar{x}_m$ - центр тяжести $S_m$ 
* $\rho{S_l, S_m}$ - расстояние между группами $S_l$ и $S_m$
* $g_i$ - эл-ты класса $S_i$. Где $i \in \{l,m\}$

## Виды расстояний в Англомеративных методах:

1. **Расстояние измеряемое по принципу ближнего соседа**(метод одиночной связи)

$$d(S_l, S_m) = min \ d(g_i, g_j),\  где \ g_i \in S_l, g_j \in S_m$$

* расстояние между кластерами это расстояние между ближайшими точками кластеров
* Не устойчив к выбросам

2. **Расстояние дальнего соседа**(метод полных связей)

$$d(S_l, S_m) = max \ d(g_i, g_j),\  где \ g_i \in S_l, g_j \in S_m$$

* расстояние между кластерами это расстояние между самыми дальними точками кластеров
* Не устойчив к выбросам

3. **Расстояние средней связи**

$$d(S_l, S_m) = \frac{1}{n_l \cdot n_m} \sum_{g_i \in S_l} \sum_{g_j \in S_m} d(g_j, g_i)$$

* ищем сумму всех попарных расстояний и делим на произведение кол-ва эл-тов в кластерах

* Учитывает взаиморасположение всех точек, за счёт чего более устойчив к выбросам чем метод ближнего и дальнего соседа

* Расстояние является промежуточным между ближним и дальним соседом

4. **Расстояние между центрами тяжести кластеров**

$$d(S_l, S_m) = d(\bar{x}_l, \bar{x}_m)$$


### Алгоритм иерархического кластерного анализа

![alt text](image-2.png)

#### Метод Уорда

здесь начало такое же, т.е на первом шаге все кластеры состоят из одного эл-та и 2 ближайших кластера объединяются.

Далее для кластеров определяется $$V_k = \sum_{i=1}^{n_k} \sum_{j=1}^{p} (x_{i,j} - \bar{x}_j)^2$$

Где:
* k - номер кластера
* i - номер объекта
* j - номер признака
* p - кол-во признаков
* $n_k$ - кол-во объектов в k-ом кластере

Далее будут объединятся такие кластеры, которые дают минимальное приращение велечины V_k, что приводит к образованию класетров с минимальной внутриклассовой вариацией.

Как и для всех Англомеративных методов в конце концов, все объекты объединяются в один кластер

Т.к суть метода Уорда состоит в том, чтобы объединять кластеры, которые дадут кластер с наименьшим $V_k$, то требуются перебрать все возможные варианты включения объектов в новых кластер, что при больших объемах данных становится невозможно


### Про Дивизимные методы классификации

Изначально все объекты принадлежат одному кластеру, далее по определенным правилам определяются схожие объекты и они откалываются в отдельный кластер. С каждым шагом кол-во кластеров растет, а мера расстояния между ними уменьшается

* Сначала мы делим глобально далекие группы, затем уже более локальные

* Пример с домашними питомцами. Сначала можно поделить их на кошечек и собачек, а затем их уже поделить на отдельные породы(Породы в рамках одного вида оч похожи и отличаются незначительными параметрами), породы на подпороды (Овчарки -> немецкая овчарка, восточно-европейкска и тд)

пример дендрограммы по дивизионному методу кластеризации
![alt text](image-3.png)