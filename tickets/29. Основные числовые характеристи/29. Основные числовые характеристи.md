# 29. Основные числовые характеристики и свойства оптимальности главных компонент в методе главных компонент.

## 1. Свойства оптимальности главных компонент

Метод главных компонент основан на переходе к новой системе координат с целью «сжатия» информации. Критерием качества (информативности) служит максимизация дисперсии новых переменных.

**Функционал информативности:**
$$I_{p'}[F(Z)] = \frac{D(F_{1})+...+D(F_{p'})}{D(Z_{1})+...+D(Z_{p})} \longrightarrow max$$

### Определение и вывод первой главной компоненты

**Определение:** Первой главной компонентой $f_1(Z)$ называется такая нормированно-центрированная линейная комбинация исходных показателей $Z$, которая обладает **наибольшей дисперсией**.
Линейная комбинация имеет вид $f_1 = Z U_1$, где $U_1$ — вектор коэффициентов.

**Доказательство (Вывод через задачу оптимизации):**
Требуется найти вектор $U_1$, максимизирующий дисперсию при условии нормировки:
$$\begin{cases} D(Z U_{1}) \longrightarrow max \\ U_{1}^{T}U_{1} = 1 \end{cases}$$

1.  **Выразим дисперсию:**
    $$D(ZU_{1}) = M[(ZU_{1})^{2}] = M[U_{1}^{T}Z^{T}ZU_{1}] = U_{1}^{T}M(Z^{T}Z)U_{1} = U_{1}^{T}R_{x}U_{1}$$
    (где $R_x$ — корреляционная матрица стандартизованных данных).
    Задача принимает вид:
    $$\begin{cases} U_{1}^{T}R_{x}U_{1} \longrightarrow max \\ U_{1}^{T}U_{1} = 1 \end{cases}$$

2.  **Функция Лагранжа:**
    Составим функцию для поиска условного экстремума:
    $$\varphi(U_{1}, \lambda) = U_{1}^{T}R_{x}U_{1} - \lambda(U_{1}^{T}U_{1} - 1)$$

3.  **Необходимое условие экстремума:**
    Найдем производную по вектору $U_1$ и приравняем её к нулю:
    $$\frac{\partial \varphi}{\partial U_{1}} = 2R_{x}U_{1} - 2\lambda U_{1} = 0$$
    Сократив на 2, получаем систему линейных однородных уравнений:
    $$R_{x}U_{1} - \lambda U_{1} = 0 \quad \text{или} \quad (R_{x} - \lambda I)U_{1} = 0$$

4.  **Решение системы:**
    Чтобы система имела ненулевое решение, определитель матрицы должен быть равен нулю:
    $$|R_{x} - \lambda I| = 0$$
    Это характеристическое уравнение. Следовательно, $\lambda$ — это собственное число матрицы $R_x$, а $U_1$ — соответствующий собственный вектор.

5.  **Максимизация дисперсии:**
    Докажем, чему равна дисперсия полученной компоненты. Умножим уравнение $R_{x}U_{1} = \lambda U_{1}$ слева на $U_{1}^{T}$:
    $$U_{1}^{T}R_{x}U_{1} = U_{1}^{T}\lambda U_{1} = \lambda (U_{1}^{T}U_{1})$$
    Так как $U_{1}^{T}U_{1} = 1$, получаем:
    $$D(f_1) = U_{1}^{T}R_{x}U_{1} = \lambda$$
    Чтобы дисперсия была **максимальной**, необходимо выбрать **наибольшее** собственное число $\lambda_1$.

**Вывод:** Первая главная компонента соответствует наибольшему собственному числу $\lambda_1$, а вектор коэффициентов $U_1$ — это собственный вектор, соответствующий $\lambda_1$.

### k-я главная компонента
Аналогично определяется $k$-я главная компонента ($f_k = Z U_k$), которая должна иметь максимальную дисперсию и **не коррелировать** с предыдущими.
Решение этой задачи приводит к тому, что $U_k$ — это собственный вектор, соответствующий $k$-му по величине собственному числу $\lambda_k$. Дисперсия этой компоненты равна $D(f_k) = \lambda_k$.

---

## 2. Основные числовые характеристики главных компонент

Определим характеристики вектора главных компонент $F = ZU$, где $U$ — матрица собственных векторов.

### 1) Математическое ожидание
$$M(F) = M(ZU) = U \cdot M(Z) = 0$$
(так как исходные данные $Z$ центрированы, $M(Z)=0$).

### 2) Ковариационная матрица (Доказательство некоррелированности)
Ковариационная матрица $S_F$ главных компонент является диагональной.

**Доказательство:**
$$S_{F} = M(F^{T}F) = M[(ZU)^{T}(ZU)] = M[U^{T}Z^{T}ZU] = U^{T}M[Z^{T}Z]U = U^{T}R_{x}U$$

Известно, что собственные векторы и числа связаны соотношением (из уравнения (10) лекции):
$$R_{x}U_{k} = \lambda_{k}U_{k}$$

Домножим это равенство слева на вектор $U_{j}^{T}$:
$$U_{j}^{T}R_{x}U_{k} = U_{j}^{T}\lambda_{k}U_{k} = \lambda_{k}(U_{j}^{T}U_{k})$$

Собственные векторы матрицы $R_x$ обладают свойствами ортогональности и нормированности:
* При $j \ne k$ (ортогональность): $U_{j}^{T}U_{k} = 0$.
* При $j = k$ (нормировка): $U_{k}^{T}U_{k} = 1$.

Следовательно:
$$U_{j}^{T}R_{x}U_{k} = \begin{cases} 0, & j \ne k \\ \lambda_{k}, & j = k \end{cases}$$

Таким образом, матрица $S_F$ имеет вид:
$$S_{F} = U^{T}R_{x}U = \begin{pmatrix} \lambda_{1} & 0 & ... & 0 \\ 0 & \lambda_{2} & ... & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & ... & \lambda_{p} \end{pmatrix}$$

Это доказывает, что главные компоненты не коррелированы (внедиагональные элементы равны нулю).

### 3) Сумма дисперсий
Сумма дисперсий всех главных компонент равна сумме дисперсий исходных признаков (равной $p$ для стандартизованных данных).

**Доказательство:**
Сумма дисперсий есть след (trace) матрицы $S_F$:
$$\sum_{k=1}^{p}D(f_{k}) = tr(S_{F}) = tr(U^{T}(R_{x}U))$$

Используем свойство следа матрицы $tr(AB) = tr(BA)$. Переставим $U^T$ в конец:
$$tr((R_{x}U)U^{T}) = tr(R_{x}(UU^{T}))$$

Так как матрица собственных векторов $U$ ортогональна, то $UU^{T} = I$ (единичная матрица):
$$= tr(R_{x}I) = tr(R_{x}) = \sum_{k=1}^{p} 1 = p$$
(так как на диагонали корреляционной матрицы $R_x$ стоят единицы).